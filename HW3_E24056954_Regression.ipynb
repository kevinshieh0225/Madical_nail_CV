{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Conv2d, MaxPool2d, Module, Flatten\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, x, y, transform):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.x[idx], cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            print('Not found img : ', self.x[idx])\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)\n",
    "        return img, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nail_csv(folder='./ML_hw2/學生的training_data/'):\n",
    "    \"\"\"\n",
    "    intro:\n",
    "        先存入原圖位置，壓縮照片後再存入./ML_hw2/學生的training_data/resize/\n",
    "        回傳 path , label\n",
    "    aug:\n",
    "        folder = 讀入資料之目的資料夾\n",
    "        batch_size = batch_size\n",
    "    output:\n",
    "        path: 照片路徑 : ./ML_hw2/resize/id\n",
    "        label: 標籤  : float number\n",
    "    \"\"\"\n",
    "    path = []\n",
    "    label = []\n",
    "    slice_csv = re.sub('學生的', \"\" ,folder.split('/')[-2] ) #提取training_data或test_data\n",
    "    csv_path = f'{folder}{slice_csv}.csv'\n",
    "    resize_folder = f'{folder}resize/'\n",
    "    if not os.path.isdir(resize_folder):\n",
    "        os.makedirs(resize_folder)\n",
    "    with open(csv_path, 'r', encoding='utf8') as f:        \n",
    "        f.readline()\n",
    "        for line in tqdm(f):\n",
    "            clean_line = line.replace('\\n', '').replace('\\ufeff', '').split(',')\n",
    "            # [id, light, ground_truth, grade]\n",
    "            curr_img_path = f'{folder}{clean_line[1]}/{clean_line[0]}'\n",
    "            new_img_path = f'{resize_folder}{clean_line[0]}'\n",
    "            if not os.path.isfile(curr_img_path):\n",
    "                print(f'No file for path : {curr_img_path}')\n",
    "                continue\n",
    "            #將未處理照片存入新資料夾位置：./ML_hw2/resize/\n",
    "            if not os.path.isfile(new_img_path):\n",
    "                img = cv2.imread(curr_img_path, cv2.IMREAD_COLOR)\n",
    "                img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "                cv2.imwrite(new_img_path, img)\n",
    "            path.append(new_img_path)\n",
    "            label.append(float(clean_line[2]))\n",
    "\n",
    "    print('data size: ')\n",
    "    print(len(path), len(label))\n",
    "    print(path[:3])\n",
    "    print(label[:3])\n",
    "    print(type(path),type(label))\n",
    "    print()\n",
    "    return path, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_prepare(folder='./ML_hw2/學生的training_data/', batch_size=8):\n",
    "    \"\"\"\n",
    "    intro:\n",
    "        使用load_nail_csv準備照片\n",
    "        Dataset轉為dataset型式\n",
    "        切 train, validation set , DataLoader存入\n",
    "    aug:\n",
    "        folder = 讀入資料之目的資料夾\n",
    "        batch_size = batch_size\n",
    "    output:\n",
    "        train_dataloader, valid_dataloader\n",
    "    \"\"\"\n",
    "\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        #torchvision.transforms.Resize((224,224)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(p = 0.5),\n",
    "        torchvision.transforms.RandomRotation(15, resample=PIL.Image.BILINEAR),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "\n",
    "    path, label = load_nail_csv(folder)\n",
    "    augment_dataset = Dataset(path, label, transform)\n",
    "    \n",
    "    #切分70%當作訓練集、30%當作驗證集\n",
    "    train_size = int(0.7 * len(augment_dataset))\n",
    "    valid_size = len(augment_dataset) - train_size\n",
    "    train_data, valid_data = torch.utils.data.random_split(augment_dataset, [train_size, valid_size])\n",
    "    \n",
    "    train_dataloader = DataLoader( train_data , batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader( valid_data , batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,name,n_epochs,train_loader,valid_loader,optimizer,criterion,batch_size,patience):\n",
    "    \"\"\"\n",
    "    intro:\n",
    "        每次epoch都 train the model , validate the model\n",
    "        印出 train_loss , val_loss \n",
    "        回傳 model\n",
    "    aug:\n",
    "        model,n_epochs,train_loader,valid_loader,optimizer,criterion,batch_size\n",
    "    output:\n",
    "        model\n",
    "    \"\"\"\n",
    "    print(f'Start to run {name}')\n",
    "#     best_train_loss = 100\n",
    "#     best_train_acc = 0\n",
    "#     best_val_loss = 100\n",
    "#     best_val_acc = 0\n",
    "#     best_F1 = 0\n",
    "#     last_epoch = 0\n",
    "\n",
    "    history = {\n",
    "        'train_loss':[],\n",
    "        'valid_loss':[],\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    else:\n",
    "        print('no gpu use')\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss,valid_loss = 0.0,0.0\n",
    "        train_losses,valid_losses=[],[]\n",
    "\n",
    "        print(f'running epoch: {epoch}/{n_epochs}')\n",
    "        #############################################################################################################\n",
    "        #                                              train the model                                              #\n",
    "        #############################################################################################################\n",
    "        model.train()\n",
    "        for num, (data, target) in enumerate(train_loader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if torch.cuda.is_available():#train_on_gpu\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            else:\n",
    "                print('1')\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output.flatten(), target.float())\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_losses.append(loss.item())\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if num%10 == 0 :\n",
    "                print(f'train stage：{num}/{len(train_loader)}', end='\\r')\n",
    "        #############################################################################################################\n",
    "        #                                            validate the model                                             #\n",
    "        #############################################################################################################\n",
    "        model.eval()\n",
    "        for num, (data, target) in enumerate(valid_loader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if torch.cuda.is_available():#train_on_gpu\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output.flatten(), target.float())\n",
    "            # update validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "                        \n",
    "            if num%10 == 0 :\n",
    "                print(f'Valid stage：{num}/{len(valid_loader)}', end='\\r')\n",
    "        #############################################################################################################\n",
    "        #                                     print train/val/cmt epoch result                                      #\n",
    "        #############################################################################################################\n",
    "        # calculate average losses\n",
    "        train_loss=np.average(train_losses)\n",
    "        valid_loss=np.average(valid_losses)\n",
    "        print(f'Training Loss: {train_loss:.3f} \\tValidation Loss: {valid_loss:.3f}\\n')\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['valid_loss'].append(valid_loss)\n",
    "        \n",
    "    \n",
    "        #############################################################################################################\n",
    "        #                                                Draw picture                                               #\n",
    "        #############################################################################################################\n",
    "    \n",
    "    with open(f'./result/{name}/result.json', 'w') as json_file:\n",
    "        json.dump(history, json_file)\n",
    "    \n",
    "    x = np.arange(1,n_epochs+1,1)\n",
    "    train_loss = history['train_loss']\n",
    "    valid_loss = history['valid_loss']\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "\n",
    "    plt.subplot(1,1,1)\n",
    "    plt.title(f\"{name} Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(x, train_loss, label='training')\n",
    "    plt.plot(x, valid_loss, label='validation')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    #save result\n",
    "    plt.savefig(f'./result/{name}/plot.png')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_result(model): \n",
    "    \"\"\"\n",
    "    intro:\n",
    "        讀取'./ML_hw2/學生的testing_data/'\n",
    "        並將照片處理，拿原本模型預測後輸出文件\n",
    "    aug:\n",
    "        model\n",
    "    result:\n",
    "        ./HW3_E24056954.csv\n",
    "    \"\"\"\n",
    "    #############################################################################################################\n",
    "    #                                   loading and resize testing picture                                      #\n",
    "    #############################################################################################################\n",
    "    testing_path = []\n",
    "    testing_write = []\n",
    "    folder = './ML_hw2/學生的testing_data/'\n",
    "    slice_csv = 'testing_data'#提取testing_data\n",
    "    csv_path = './HW2_E24056954.csv'\n",
    "    resize_folder = f'{folder}resize/'\n",
    "    if not os.path.isdir(resize_folder):\n",
    "        os.makedirs(resize_folder)\n",
    "    with open(csv_path, 'r', encoding='utf8') as f:   \n",
    "        testing_write.append(f.readline())\n",
    "        for line in f:\n",
    "            clean_line = line.replace('\\n', '').replace('\\ufeff', '').split(',')\n",
    "            # [id, light, ground_truth, grade]\n",
    "            testing_write.append(clean_line)\n",
    "            curr_img_path = f'{folder}{slice_csv}/{clean_line[0]}'\n",
    "            new_img_path = f'{resize_folder}{clean_line[0]}'\n",
    "            if not os.path.isfile(curr_img_path):\n",
    "                print(curr_img_path)\n",
    "                print('catch')\n",
    "                continue\n",
    "            if not os.path.isfile(new_img_path):\n",
    "                img = cv2.imread(curr_img_path, cv2.IMREAD_COLOR)\n",
    "                img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "                cv2.imwrite(new_img_path, img)\n",
    "            testing_path.append(new_img_path)\n",
    "    print('data size: ')\n",
    "    print(f'testing數量：{len(testing_path)}')\n",
    "    \n",
    "    #############################################################################################################\n",
    "    #                                   use hypothesis model predict testing set                                #\n",
    "    #############################################################################################################\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "    model.eval()\n",
    "    pred_regression=[]\n",
    "    for path in testing_path:\n",
    "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        img = transform(img).cuda()\n",
    "        img = img.unsqueeze(0)\n",
    "        with torch.no_grad(): \n",
    "            output=model(img)\n",
    "        output = float(output.squeeze(0)[0])\n",
    "        pred_regression.append(output)\n",
    "        print(f'{path} / {output}')\n",
    "    #############################################################################################################\n",
    "    #                                             output require csv                                            #\n",
    "    #############################################################################################################\n",
    "    with open('HW3_E24056954.csv', 'w', encoding='utf8') as wp:\n",
    "        wp.write(testing_write[0])\n",
    "        for pred_regression_,testing_write_ in zip(pred_regression,testing_write[1:]):\n",
    "            wp.write(f'{testing_write_[0]},{testing_write_[1]},{pred_regression_},{testing_write_[3]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try wide_resnet50_2 finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'HW3_model_ft_wide_resnet50_2'\n",
    "model_ft = models.wide_resnet50_2(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try vgg19 finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'HW3_model_ft_vgg19'\n",
    "# model_ft = models.vgg19(pretrained=True)\n",
    "# num_ftrs = model_ft.classifier[6].in_features\n",
    "# model_ft.classifier[6] = nn.Linear(num_ftrs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU State: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2d559421e04a018d9750edd2493cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "['./ML_hw2/學生的training_data/resize/00130747_A_3457_20200715_100727_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_3458_20200715_100736_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4810_20200812_112534_7.9.jpg']\n",
      "[7.5, 7.5, 7.9]\n",
      "<class 'list'> <class 'list'>\n",
      "\n",
      "Start to run HW3_model_ft_wide_resnet50_2\n",
      "running epoch: 1/12\n",
      "train stage：0/45\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.76 GiB total capacity; 3.79 GiB already allocated; 8.25 MiB free; 3.87 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4e5c393e9792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       patience)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-8b0744bd044a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, name, n_epochs, train_loader, valid_loader, optimizer, criterion, batch_size, patience)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;31m# calculate the batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1922\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m     )\n\u001b[1;32m   1925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.76 GiB total capacity; 3.79 GiB already allocated; 8.25 MiB free; 3.87 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('GPU State:', device)\n",
    "model_ft=model_ft.to(device)# 放入裝置\n",
    "model_ft = model_ft.float()\n",
    "n_epochs = 12\n",
    "batch_size = 32\n",
    "train_dataloader, valid_dataloader = dataloader_prepare(folder='./ML_hw2/學生的training_data/',batch_size = batch_size)\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':model_ft.parameters()}\n",
    "], lr=0.0001)\n",
    "criterion = nn.MSELoss()\n",
    "patience = 3\n",
    "if not os.path.isdir(f'./result/{name}/'):\n",
    "    os.makedirs(f'./result/{name}/')\n",
    "model_ft = train(model_ft,\n",
    "      name,\n",
    "      n_epochs,\n",
    "      train_dataloader,\n",
    "      valid_dataloader,\n",
    "      optimizer,\n",
    "      criterion,\n",
    "      batch_size,\n",
    "      patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print testing data result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_result(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test CNN + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2384502d4f3e46cc94649cb70fafa990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "['./ML_hw2/學生的training_data/resize/00130747_A_3457_20200715_100727_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_3458_20200715_100736_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4810_20200812_112534_7.9.jpg']\n",
      "[7.5, 7.5, 7.9]\n",
      "<class 'list'> <class 'list'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fedf178a1b492795e6529f783992e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f1a679c27946fc856f95bf2d3db574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=406.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11:51:01] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:51:04] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MSE : 0.5993374042116644\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637d7bc6af224b4c8ad4367df383162c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "['./ML_hw2/學生的training_data/resize/00130747_A_3457_20200715_100727_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_3458_20200715_100736_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4810_20200812_112534_7.9.jpg']\n",
      "[7.5, 7.5, 7.9]\n",
      "<class 'list'> <class 'list'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1db984fde7540b38c5647a5cbf23c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8bdb3bb8d84620bbabb20d46e9e385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=406.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11:51:24] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:51:27] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MSE : 0.5566511014432637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c2d6f5f17149ada0a587cdee02bf13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "['./ML_hw2/學生的training_data/resize/00130747_A_3457_20200715_100727_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_3458_20200715_100736_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4810_20200812_112534_7.9.jpg']\n",
      "[7.5, 7.5, 7.9]\n",
      "<class 'list'> <class 'list'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bef1f7023747a4a929f1127b32c0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343dcc5f3573419a8dd1d9111caad7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=406.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11:51:47] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:51:50] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MSE : 0.5695938448414936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d7e780387b4dce8a945938b13513ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "['./ML_hw2/學生的training_data/resize/00130747_A_3457_20200715_100727_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_3458_20200715_100736_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4810_20200812_112534_7.9.jpg']\n",
      "[7.5, 7.5, 7.9]\n",
      "<class 'list'> <class 'list'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a9c65d4fea4fd1987f0afe1f40ec67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767d45489b554d05a5a911d5a91c4009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=406.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11:52:10] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:14] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MSE : 0.6484847049546669\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ca19f0ccce4ccd8ab1cfed2ee5f5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "['./ML_hw2/學生的training_data/resize/00130747_A_3457_20200715_100727_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_3458_20200715_100736_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4810_20200812_112534_7.9.jpg']\n",
      "[7.5, 7.5, 7.9]\n",
      "<class 'list'> <class 'list'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7a8f9418f9496c83e47505d1d835d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3118c8cf203344e998471c6ae5e6f62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=406.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11:52:34] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:37] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MSE : 0.6328475406123292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e5f7eceab74acda8784c3a298f28fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "['./ML_hw2/學生的training_data/resize/00130747_A_3457_20200715_100727_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_3458_20200715_100736_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4810_20200812_112534_7.9.jpg']\n",
      "[7.5, 7.5, 7.9]\n",
      "<class 'list'> <class 'list'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee10b28b79d4cca87359d864d3ac01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81aab71ac9094db9ba68f9555d7a109f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=406.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11:52:59] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:53:02] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MSE : 0.6771308866682039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541a7222df204db7ba18aa5cfed0ce60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "['./ML_hw2/學生的training_data/resize/00130747_A_3457_20200715_100727_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_3458_20200715_100736_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4810_20200812_112534_7.9.jpg']\n",
      "[7.5, 7.5, 7.9]\n",
      "<class 'list'> <class 'list'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14020603426a471eb7b55f99fcf7da04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a08ea1ebf44fd997a82068bb8d26fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=406.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11:53:23] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:53:26] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MSE : 0.6319547735996011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3e0279e23148c1b57288ad4bcf6d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "['./ML_hw2/學生的training_data/resize/00130747_A_3457_20200715_100727_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_3458_20200715_100736_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4810_20200812_112534_7.9.jpg']\n",
      "[7.5, 7.5, 7.9]\n",
      "<class 'list'> <class 'list'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91057e2ba42f4ab7be7c2580495e6a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30bbe04f37b4f04b82f216b5c4325c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=406.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11:53:47] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:53:50] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MSE : 0.519745091898487\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20abe42081524f76b54bbb12869f4285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "['./ML_hw2/學生的training_data/resize/00130747_A_3457_20200715_100727_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_3458_20200715_100736_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4810_20200812_112534_7.9.jpg']\n",
      "[7.5, 7.5, 7.9]\n",
      "<class 'list'> <class 'list'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4635df2a41c5484e935aefd434858a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e3cdb18cf7451f8a64c3428417e4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=406.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11:54:12] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:54:15] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MSE : 0.5568559526796997\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa8ac49e22440ac8892f6687dfd96d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "['./ML_hw2/學生的training_data/resize/00130747_A_3457_20200715_100727_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_3458_20200715_100736_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4810_20200812_112534_7.9.jpg']\n",
      "[7.5, 7.5, 7.9]\n",
      "<class 'list'> <class 'list'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ccb3574f0d4ce8aa74c500d785af87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e151bbbb934889812c103e4f5abe84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=406.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11:54:37] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:54:40] WARNING: /workspace/src/objective/regression_obj.cu:168: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MSE : 0.6048103722101078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import xgboost as xg \n",
    "from sklearn.metrics import mean_squared_error as MSE \n",
    "\n",
    "model_cnn = torch.load('./result/HW3_model_ft_wide_resnet50_2/model.pt')\n",
    "model_cnn_extractor = nn.Sequential(*list(model_cnn.children())[:-1]) # strips off last linear layer\n",
    "\n",
    "\n",
    "\n",
    "def trainsform_data( address_set , transform , model):\n",
    "    data = []\n",
    "    for add in tqdm(address_set):\n",
    "        img = cv2.imread(add, cv2.IMREAD_COLOR)\n",
    "        img = transform(img).cuda()\n",
    "        img = img.unsqueeze(0)\n",
    "        with torch.no_grad(): \n",
    "            output=model(img)\n",
    "        output = output.squeeze(0).squeeze(1).squeeze(1).tolist()\n",
    "        data.append(output)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "for seed in range(10):\n",
    "    seed = seed*10\n",
    "    add_x , label_y = load_nail_csv(folder='./ML_hw2/學生的training_data/')\n",
    "    train_add_x , test_add_x , train_y , test_y = train_test_split(add_x,label_y,test_size=0.2,random_state=seed)\n",
    "\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "\n",
    "    train_X = trainsform_data( train_add_x , transform , model_cnn_extractor)\n",
    "    test_X = trainsform_data( test_add_x , transform , model_cnn_extractor)\n",
    "    train_y = pd.DataFrame(train_y)\n",
    "    test_y = pd.DataFrame(test_y)\n",
    "#     print(f'train_X len:{len(train_X)}')\n",
    "#     print(f'train_y len:{len(train_y)}')\n",
    "#     print(f'test_X len:{len(test_X)}')\n",
    "#     print(f'test_y len:{len(test_y)}')\n",
    "\n",
    "#     print(f'train_X type:{type(train_X)}')\n",
    "#     print(f'train_y type:{type(train_y)}')\n",
    "#     print(f'test_X type:{type(test_X)}')\n",
    "#     print(f'test_y type:{type(test_y)}')\n",
    "\n",
    "    # Instantiation \n",
    "    xgb_r = xg.XGBRegressor(objective ='reg:linear', \n",
    "                      n_estimators = 100, seed = 50) \n",
    "\n",
    "    # Fitting the model \n",
    "    xgb_r.fit(train_X, train_y) \n",
    "\n",
    "\n",
    "    # Predict the model \n",
    "    pred = xgb_r.predict(test_X) \n",
    "\n",
    "    # for pred_ , test_ in zip(pred,test_y[0]):\n",
    "    #     print(f'pred , test : {pred_:.3f}\\t{test_}')\n",
    "\n",
    "    # RMSE Computation \n",
    "    mse = MSE(test_y, pred)\n",
    "    print(f\"MSE : {mse}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_result_cnn_xgb(XGR , model_extraction , folder = './testing_data_part2/' , csv_path = './testing_data_part2.csv'): \n",
    "    \"\"\"\n",
    "    intro:\n",
    "        讀取'./ML_hw2/學生的testing_data/'\n",
    "        並將照片處理，拿原本模型預測後輸出文件\n",
    "    aug:\n",
    "        model\n",
    "    result:\n",
    "        ./testing_data_part2.csv\n",
    "    \"\"\"\n",
    "    #############################################################################################################\n",
    "    #                                   loading and resize testing picture                                      #\n",
    "    #############################################################################################################\n",
    "    testing_path = []\n",
    "    testing_write = []\n",
    "    resize_folder = f'{folder}resize/'\n",
    "    if not os.path.isdir(resize_folder):\n",
    "        os.makedirs(resize_folder)\n",
    "    with open(csv_path, 'r', encoding='utf8') as f:   \n",
    "        testing_write.append(f.readline())\n",
    "        for line in f:\n",
    "            clean_line = line.replace('\\n', '').split(',')\n",
    "            # [id, light, ground_truth, grade]\n",
    "            testing_write.append(clean_line)\n",
    "            curr_img_path = f'{folder}/{clean_line[0]}'\n",
    "            new_img_path = f'{resize_folder}{clean_line[0]}'\n",
    "            if not os.path.isfile(curr_img_path):\n",
    "                print(curr_img_path)\n",
    "                print('catch')\n",
    "                continue\n",
    "            if not os.path.isfile(new_img_path):\n",
    "                img = cv2.imread(curr_img_path, cv2.IMREAD_COLOR)\n",
    "                img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "                cv2.imwrite(new_img_path, img)\n",
    "            testing_path.append(new_img_path)\n",
    "    print('data size: ')\n",
    "    print(f'testing數量：{len(testing_path)}')\n",
    "    print(testing_path[:5])\n",
    "    #############################################################################################################\n",
    "    #                                               cnn extraction                                              #\n",
    "    #############################################################################################################\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "    test_X = trainsform_data( testing_path , transform , model_cnn_extractor)\n",
    "    \n",
    "    #############################################################################################################\n",
    "    #                                             xgboost prediction                                            #\n",
    "    #############################################################################################################\n",
    "    pred = XGR.predict(test_X) \n",
    "    #############################################################################################################\n",
    "    #                                             output require csv                                            #\n",
    "    #############################################################################################################\n",
    "    with open('HW3_CNN_XGB_E24056954.csv', 'w', encoding='utf8') as wp:\n",
    "        wp.write(testing_write[0])\n",
    "        for pred_regression_,testing_write_ in zip(pred,testing_write[1:]):\n",
    "            wp.write(f'{testing_write_[0]},{pred_regression_}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import xgboost as xg \n",
    "from sklearn.metrics import mean_squared_error as MSE \n",
    "\n",
    "model_cnn = torch.load('./result/HW3_model_ft_wide_resnet50_2/model.pt')\n",
    "model_cnn_extractor = nn.Sequential(*list(model_cnn.children())[:-1]) # strips off last linear layer\n",
    "\n",
    "\n",
    "\n",
    "def trainsform_data( address_set , transform , model):\n",
    "    data = []\n",
    "    for add in tqdm(address_set):\n",
    "        img = cv2.imread(add, cv2.IMREAD_COLOR)\n",
    "        img = transform(img).cuda()\n",
    "        img = img.unsqueeze(0)\n",
    "        with torch.no_grad(): \n",
    "            output=model(img)\n",
    "        output = output.squeeze(0).squeeze(1).squeeze(1).tolist()\n",
    "        data.append(output)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "add_x , label_y = load_nail_csv(folder='./ML_hw2/學生的training_data/')\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "train_X = trainsform_data( add_x , transform , model_cnn_extractor)\n",
    "train_y = pd.DataFrame(label_y)\n",
    "#     print(f'train_X len:{len(train_X)}')\n",
    "#     print(f'train_y len:{len(train_y)}')\n",
    "#     print(f'test_X len:{len(test_X)}')\n",
    "#     print(f'test_y len:{len(test_y)}')\n",
    "\n",
    "#     print(f'train_X type:{type(train_X)}')\n",
    "#     print(f'train_y type:{type(train_y)}')\n",
    "#     print(f'test_X type:{type(test_X)}')\n",
    "#     print(f'test_y type:{type(test_y)}')\n",
    "\n",
    "# Instantiation \n",
    "xgb_r = xg.XGBRegressor(objective ='reg:linear', \n",
    "                  n_estimators = 100, seed = 50) \n",
    "\n",
    "# Fitting the model \n",
    "xgb_r.fit(train_X, train_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: \n",
      "testing數量：534\n",
      "['./testing_data_part2/resize/wslOYFyS.jpg', './testing_data_part2/resize/mjEZWfu2.jpg', './testing_data_part2/resize/0d2zTSnx.jpg', './testing_data_part2/resize/cvzGTNxF.jpg', './testing_data_part2/resize/DYqR4jzb.jpg']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d22342918c430ab0796e5cde145cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=534.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testing_result_cnn_xgb(xgb_r , model_cnn_extractor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
