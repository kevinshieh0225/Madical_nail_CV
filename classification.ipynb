{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "#import h5py\n",
    "\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Conv2d, MaxPool2d, Module\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, x, y, transform):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.x[idx], cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            print('Not found img : ', self.x[idx])\n",
    "            print(self.x[idx])\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)\n",
    "        return img, self.y[idx]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nail_csv(folder='./ML_hw2/學生的training_data/'):\n",
    "    \"\"\"\n",
    "    intro:\n",
    "        先存入原圖位置，壓縮照片後再存入./ML_hw2/學生的training_data/resize/\n",
    "        回傳 path , label\n",
    "    aug:\n",
    "        folder = 讀入資料之目的資料夾\n",
    "        batch_size = batch_size\n",
    "    output:\n",
    "        path: 照片路徑 : ./ML_hw2/resize/id\n",
    "        label: 標籤  : [1,0,0]\n",
    "    \"\"\"\n",
    "    path = []\n",
    "    label = []\n",
    "    slice_csv = re.sub('學生的', \"\" ,folder.split('/')[-2] ) #提取training_data或test_data\n",
    "    csv_path = f'{folder}{slice_csv}.csv'\n",
    "    resize_folder = f'{folder}resize/'\n",
    "    if not os.path.isdir(resize_folder):\n",
    "        os.makedirs(resize_folder)\n",
    "    with open(csv_path, 'r', encoding='utf8') as f:        \n",
    "        f.readline()\n",
    "        for line in tqdm(f):\n",
    "            clean_line = line.replace('\\n', '').replace('\\ufeff', '').split(',')\n",
    "            # [id, light, ground_truth, grade]\n",
    "            curr_img_path = f'{folder}{clean_line[1]}/{clean_line[0]}'\n",
    "            new_img_path = f'{resize_folder}{clean_line[0]}'\n",
    "            if not os.path.isfile(curr_img_path):\n",
    "                print(f'No file for path : {curr_img_path}')\n",
    "                continue\n",
    "            if not os.path.isfile(new_img_path):\n",
    "                img = cv2.imread(curr_img_path, cv2.IMREAD_COLOR)\n",
    "                img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "                cv2.imwrite(new_img_path, img)\n",
    "            path.append(new_img_path)\n",
    "            label.append(int(clean_line[3])-1)\n",
    "\n",
    "    print('data size: ')\n",
    "    print(len(path), len(label))\n",
    "    count = np.zeros(3)\n",
    "    for check in label:\n",
    "        count[int(check)] += 1\n",
    "    print('(1)：'+str(count[0])+' '+str(count[0]/len(label)))\n",
    "    print('(2)：'+str(count[1])+' '+str(count[1]/len(label)))\n",
    "    print('(3)：'+str(count[2])+' '+str(count[2]/len(label)))\n",
    "\n",
    "    print()\n",
    "    return path, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_prepare(folder='./ML_hw2/學生的training_data/', batch_size=8):\n",
    "    \"\"\"\n",
    "    intro:\n",
    "        使用load_nail_csv準備照片\n",
    "        Dataset轉為dataset型式\n",
    "        切 train, validation set , DataLoader存入\n",
    "    aug:\n",
    "        folder = 讀入資料之目的資料夾\n",
    "        batch_size = batch_size\n",
    "    output:\n",
    "        train_dataloader, valid_dataloader\n",
    "    \"\"\"\n",
    "\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        #torchvision.transforms.Resize((224,224)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(p = 0.5),\n",
    "        torchvision.transforms.RandomRotation(15, resample=PIL.Image.BILINEAR),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "\n",
    "    path, label = load_nail_csv(folder)\n",
    "\n",
    "    augment_dataset = Dataset(path, label, transform)\n",
    "    #切分70%當作訓練集、30%當作驗證集\n",
    "    train_size = int(0.7 * len(augment_dataset))\n",
    "    valid_size = len(augment_dataset) - train_size\n",
    "    train_data, valid_data = torch.utils.data.random_split(augment_dataset, [train_size, valid_size])\n",
    "    \n",
    "    train_dataloader = DataLoader( train_data , batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader( valid_data , batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU State: cuda:0\n",
      "CNN_Model(\n",
      "  (CNN): Sequential(\n",
      "    (0): Conv2d(3, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(50, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=140450, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=3, bias=True)\n",
      "    (3): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    #列出需要哪些層\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.CNN = nn.Sequential(                       #(3,224,224)\n",
    "            nn.Conv2d(3, 50, kernel_size=5, stride=1),  #(50,220,220)\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 1\n",
    "            nn.MaxPool2d(kernel_size=2),                #(50,110,110)\n",
    "            # Convolution 2\n",
    "            nn.Conv2d(50,50, kernel_size=5, stride=1),  #(50,106,106)\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2\n",
    "            nn.MaxPool2d(kernel_size=2)                 #(50,53,53)\n",
    "            # Fully connected 1 ,#input_shape=(8*53*53)\n",
    "        )\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=50*53*53, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=3),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.CNN(input)\n",
    "        out = out.view(out.size(0), -1) \n",
    "        return self.main(out)\n",
    "   \n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('GPU State:', device)\n",
    "net = CNN_Model().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,n_epochs,train_loader,valid_loader,optimizer,criterion,batch_size,patience):\n",
    "    \"\"\"\n",
    "    intro:\n",
    "        每次epoch都 train the model , validate the model\n",
    "        並計算Early Stopping\n",
    "        印出 train_loss , train_acc , val_loss , val_acc\n",
    "        回傳 model\n",
    "    aug:\n",
    "        model,n_epochs,train_loader,valid_loader,optimizer,criterion,batch_size\n",
    "    output:\n",
    "        model\n",
    "    \"\"\"\n",
    "    the_last_loss = 100\n",
    "    trigger_times = 0\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    else:\n",
    "        print('no gpu use')\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # keep track of training and validation loss\n",
    "        train_loss,valid_loss = 0.0,0.0\n",
    "        train_losses,valid_losses=[],[]\n",
    "        train_correct,val_correct,train_total,val_total=0,0,0,0\n",
    "        train_pred,train_target=torch.zeros(batch_size,1),torch.zeros(batch_size,1)\n",
    "        val_pred,val_target=torch.zeros(batch_size,1),torch.zeros(batch_size,1)\n",
    "        count=0\n",
    "        count2=0\n",
    "        print('running epoch: {}'.format(epoch))\n",
    "        #############################################################################################################\n",
    "        #                                              train the model                                              #\n",
    "        #############################################################################################################\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if torch.cuda.is_available():#train_on_gpu\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            else:\n",
    "                print('1')\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            #calculate accuracy\n",
    "            pred = output.data.max(dim = 1, keepdim = True)[1]\n",
    "            \n",
    "            train_correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "            train_total += data.size(0)\n",
    "            \n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_losses.append(loss.item()*data.size(0))\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            if count==0:\n",
    "                train_pred=pred\n",
    "                train_target=target.data.view_as(pred)\n",
    "                count=count+1\n",
    "            else:\n",
    "                train_pred=torch.cat((train_pred,pred), 0)\n",
    "                train_target=torch.cat((train_target,target.data.view_as(pred)), 0)\n",
    "        train_pred=train_pred.cpu().view(-1).numpy().tolist()\n",
    "        train_target=train_target.cpu().view(-1).numpy().tolist()\n",
    "        \n",
    "        #############################################################################################################\n",
    "        #                                            validate the model                                             #\n",
    "        #############################################################################################################\n",
    "        model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if torch.cuda.is_available():#train_on_gpu\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss =criterion(output, target)\n",
    "            #calculate accuracy\n",
    "            pred = output.data.max(dim = 1, keepdim = True)[1]\n",
    "            val_correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "            val_total += data.size(0)\n",
    "            valid_losses.append(loss.item()*data.size(0))\n",
    "            if count2==0:\n",
    "                val_pred=pred\n",
    "                val_target=target.data.view_as(pred)\n",
    "                count2=count+1\n",
    "            else:\n",
    "                val_pred=torch.cat((val_pred,pred), 0)\n",
    "                val_target=torch.cat((val_target,target.data.view_as(pred)), 0)\n",
    "        val_pred=val_pred.cpu().view(-1).numpy().tolist()\n",
    "        val_target=val_target.cpu().view(-1).numpy().tolist()\n",
    "        \n",
    "        # calculate average losses\n",
    "        train_loss=np.average(train_losses)\n",
    "        valid_loss=np.average(valid_losses)\n",
    "        \n",
    "        #############################################################################################################\n",
    "        #                                        calculate average accuracy                                         #\n",
    "        #############################################################################################################\n",
    "        train_acc=train_correct/train_total\n",
    "        valid_acc=val_correct/val_total\n",
    "        print(f'\\tTraining Loss: {train_loss:.6f} \\tValidation Loss: {valid_loss:.6f}')\n",
    "        print(f'\\tTraining Accuracy: {train_acc:.6f} \\tValidation Accuracy: {valid_acc:.6f}')\n",
    "        \n",
    "        #############################################################################################################\n",
    "        #                                              Early Stopping                                               #\n",
    "        #############################################################################################################\n",
    "        if valid_loss > the_last_loss:\n",
    "            trigger_times += 1\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                return model\n",
    "\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "        the_last_loss = valid_loss\n",
    "        print(f'trigger times: {trigger_times}\\n')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU State: cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print('GPU State:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cbc71274c94b4a9905d77a47a7ef79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catch\n",
      "catch\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "['./ML_hw2/學生的training_data/resize/00130747_A_3457_20200715_100727_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_3458_20200715_100736_7.5.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4810_20200812_112534_7.9.jpg', './ML_hw2/學生的training_data/resize/00130747_A_4811_20200812_112539_7.9.jpg', './ML_hw2/學生的training_data/resize/00241567_A_6659_20200916_153156_7.6.jpg', './ML_hw2/學生的training_data/resize/00241567_A_6660_20200916_153202_7.6.jpg', './ML_hw2/學生的training_data/resize/00241567_A_7643_20200930_104310_7.8.jpg', './ML_hw2/學生的training_data/resize/00241567_A_7644_20200930_104313_7.8.jpg', './ML_hw2/學生的training_data/resize/00311620_A_3097_20200710_102137_13.5.jpg', './ML_hw2/學生的training_data/resize/00311620_A_3098_20200710_102142_13.5.jpg']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 2, 2]\n",
      "(1)：377.0 0.18608094768015795\n",
      "(2)：488.0 0.24086870681145114\n",
      "(3)：1161.0 0.573050345508391\n",
      "\n",
      "augment data len : 2026\n",
      "augment data type : <__main__.Dataset object at 0x7f5bd3ba9780>\n",
      "<torch.utils.data.dataset.Subset object at 0x7f5bc06b8a58>\n",
      "augment data len : 1418\n",
      "augment data type : <torch.utils.data.dataset.Subset object at 0x7f5bc06b8a58>\n",
      "<torch.utils.data.dataset.Subset object at 0x7f5bc06b89b0>\n",
      "augment data len : 608\n",
      "augment data type : <torch.utils.data.dataset.Subset object at 0x7f5bc06b89b0>\n",
      "<torch.utils.data.dataset.Subset object at 0x7f5bc06b8a58>\n",
      "augment data len : 45\n",
      "augment data type : <torch.utils.data.dataloader.DataLoader object at 0x7f5bc06b80f0>\n",
      "<torch.utils.data.dataset.Subset object at 0x7f5bc06b89b0>\n",
      "augment data len : 19\n",
      "augment data type : <torch.utils.data.dataloader.DataLoader object at 0x7f5bc06b8d68>\n",
      "running epoch: 1\n",
      "trigger times: 0\n",
      "\tTraining Loss: 33.527982 \tValidation Loss: 31.401269\n",
      "\tTraining Accuracy: 0.562764 \tValidation Accuracy: 0.546053\n",
      "running epoch: 2\n",
      "trigger times: 0\n",
      "\tTraining Loss: 29.466775 \tValidation Loss: 30.244683\n",
      "\tTraining Accuracy: 0.584626 \tValidation Accuracy: 0.546053\n",
      "running epoch: 3\n",
      "trigger times: 1\n",
      "\tTraining Loss: 27.745937 \tValidation Loss: 31.256033\n",
      "\tTraining Accuracy: 0.597320 \tValidation Accuracy: 0.547697\n",
      "running epoch: 4\n",
      "trigger times: 0\n",
      "\tTraining Loss: 25.733683 \tValidation Loss: 28.341538\n",
      "\tTraining Accuracy: 0.634697 \tValidation Accuracy: 0.550987\n",
      "running epoch: 5\n",
      "trigger times: 0\n",
      "\tTraining Loss: 24.941348 \tValidation Loss: 26.561518\n",
      "\tTraining Accuracy: 0.643159 \tValidation Accuracy: 0.620066\n",
      "running epoch: 6\n",
      "trigger times: 0\n",
      "\tTraining Loss: 23.986387 \tValidation Loss: 26.237062\n",
      "\tTraining Accuracy: 0.664316 \tValidation Accuracy: 0.611842\n",
      "running epoch: 7\n",
      "trigger times: 1\n",
      "\tTraining Loss: 22.829321 \tValidation Loss: 26.872329\n",
      "\tTraining Accuracy: 0.680536 \tValidation Accuracy: 0.611842\n",
      "running epoch: 8\n",
      "trigger times: 0\n",
      "\tTraining Loss: 22.555401 \tValidation Loss: 26.075337\n",
      "\tTraining Accuracy: 0.689704 \tValidation Accuracy: 0.626645\n",
      "running epoch: 9\n",
      "trigger times: 1\n",
      "\tTraining Loss: 22.946446 \tValidation Loss: 26.234178\n",
      "\tTraining Accuracy: 0.661495 \tValidation Accuracy: 0.575658\n",
      "running epoch: 10\n",
      "trigger times: 2\n",
      "\tTraining Loss: 21.620026 \tValidation Loss: 27.606225\n",
      "\tTraining Accuracy: 0.697461 \tValidation Accuracy: 0.544408\n",
      "running epoch: 11\n",
      "trigger times: 0\n",
      "\tTraining Loss: 21.709867 \tValidation Loss: 27.368657\n",
      "\tTraining Accuracy: 0.700282 \tValidation Accuracy: 0.598684\n",
      "running epoch: 12\n",
      "trigger times: 0\n",
      "\tTraining Loss: 20.865682 \tValidation Loss: 25.881633\n",
      "\tTraining Accuracy: 0.718618 \tValidation Accuracy: 0.610197\n",
      "running epoch: 13\n",
      "trigger times: 1\n",
      "\tTraining Loss: 19.767575 \tValidation Loss: 28.358478\n",
      "\tTraining Accuracy: 0.736953 \tValidation Accuracy: 0.608553\n",
      "running epoch: 14\n",
      "trigger times: 0\n",
      "\tTraining Loss: 19.807125 \tValidation Loss: 28.091895\n",
      "\tTraining Accuracy: 0.742595 \tValidation Accuracy: 0.618421\n",
      "running epoch: 15\n",
      "trigger times: 0\n",
      "\tTraining Loss: 19.137657 \tValidation Loss: 25.763491\n",
      "\tTraining Accuracy: 0.736953 \tValidation Accuracy: 0.626645\n",
      "running epoch: 16\n",
      "trigger times: 1\n",
      "\tTraining Loss: 18.664800 \tValidation Loss: 26.823913\n",
      "\tTraining Accuracy: 0.744711 \tValidation Accuracy: 0.615132\n",
      "running epoch: 17\n",
      "trigger times: 2\n",
      "\tTraining Loss: 18.498909 \tValidation Loss: 28.754392\n",
      "\tTraining Accuracy: 0.742595 \tValidation Accuracy: 0.625000\n",
      "running epoch: 18\n",
      "trigger times: 0\n",
      "\tTraining Loss: 20.187917 \tValidation Loss: 26.335411\n",
      "\tTraining Accuracy: 0.724965 \tValidation Accuracy: 0.628289\n",
      "running epoch: 19\n",
      "trigger times: 0\n",
      "\tTraining Loss: 17.112934 \tValidation Loss: 25.890938\n",
      "\tTraining Accuracy: 0.780677 \tValidation Accuracy: 0.648026\n",
      "running epoch: 20\n",
      "trigger times: 1\n",
      "\tTraining Loss: 16.214978 \tValidation Loss: 28.462635\n",
      "\tTraining Accuracy: 0.782087 \tValidation Accuracy: 0.615132\n",
      "running epoch: 21\n",
      "trigger times: 0\n",
      "\tTraining Loss: 15.321574 \tValidation Loss: 28.370933\n",
      "\tTraining Accuracy: 0.818054 \tValidation Accuracy: 0.633224\n",
      "running epoch: 22\n",
      "trigger times: 0\n",
      "\tTraining Loss: 15.963987 \tValidation Loss: 26.584802\n",
      "\tTraining Accuracy: 0.795487 \tValidation Accuracy: 0.615132\n",
      "running epoch: 23\n",
      "trigger times: 1\n",
      "\tTraining Loss: 14.242197 \tValidation Loss: 27.054398\n",
      "\tTraining Accuracy: 0.811707 \tValidation Accuracy: 0.618421\n",
      "running epoch: 24\n",
      "trigger times: 0\n",
      "\tTraining Loss: 14.440184 \tValidation Loss: 26.171147\n",
      "\tTraining Accuracy: 0.815233 \tValidation Accuracy: 0.639803\n",
      "running epoch: 25\n",
      "trigger times: 1\n",
      "\tTraining Loss: 13.881683 \tValidation Loss: 27.501899\n",
      "\tTraining Accuracy: 0.817348 \tValidation Accuracy: 0.651316\n",
      "running epoch: 26\n",
      "trigger times: 2\n",
      "\tTraining Loss: 12.786637 \tValidation Loss: 29.531163\n",
      "\tTraining Accuracy: 0.846968 \tValidation Accuracy: 0.649671\n",
      "running epoch: 27\n",
      "trigger times: 3\n",
      "Early stopping!\n",
      "Start to test process.\n"
     ]
    }
   ],
   "source": [
    "model1=CNN_Model()\n",
    "n_epochs = 100\n",
    "LR = 0.0001\n",
    "batch_size = 32\n",
    "train_dataloader, valid_dataloader = dataloader_prepare(folder='./ML_hw2/學生的training_data/',batch_size = 32)\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=LR)\n",
    "criterion = CrossEntropyLoss()\n",
    "patience = 3\n",
    "model1=train(model1,n_epochs,train_dataloader,valid_dataloader,optimizer1,criterion, batch_size, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try alexnet finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "(1)：377.0 0.18608094768015795\n",
      "(2)：488.0 0.24086870681145114\n",
      "(3)：1161.0 0.573050345508391\n",
      "\n",
      "running epoch: 1\n",
      "\tTraining Loss: 31.734260 \tValidation Loss: 30.213683\n",
      "\tTraining Accuracy: 0.559238 \tValidation Accuracy: 0.575658\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 2\n",
      "\tTraining Loss: 30.152368 \tValidation Loss: 29.694429\n",
      "\tTraining Accuracy: 0.559944 \tValidation Accuracy: 0.582237\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 3\n",
      "\tTraining Loss: 29.212920 \tValidation Loss: 28.951340\n",
      "\tTraining Accuracy: 0.578984 \tValidation Accuracy: 0.578947\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 4\n",
      "\tTraining Loss: 28.468965 \tValidation Loss: 28.171375\n",
      "\tTraining Accuracy: 0.590973 \tValidation Accuracy: 0.587171\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 5\n",
      "\tTraining Loss: 27.453534 \tValidation Loss: 27.238649\n",
      "\tTraining Accuracy: 0.600846 \tValidation Accuracy: 0.595395\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 6\n",
      "\tTraining Loss: 26.668759 \tValidation Loss: 26.428065\n",
      "\tTraining Accuracy: 0.595205 \tValidation Accuracy: 0.606908\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 7\n",
      "\tTraining Loss: 24.752030 \tValidation Loss: 24.834123\n",
      "\tTraining Accuracy: 0.641749 \tValidation Accuracy: 0.644737\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 8\n",
      "\tTraining Loss: 23.344394 \tValidation Loss: 24.340775\n",
      "\tTraining Accuracy: 0.675599 \tValidation Accuracy: 0.666118\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 9\n",
      "\tTraining Loss: 22.603246 \tValidation Loss: 24.394073\n",
      "\tTraining Accuracy: 0.684767 \tValidation Accuracy: 0.644737\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 10\n",
      "\tTraining Loss: 21.568248 \tValidation Loss: 23.454266\n",
      "\tTraining Accuracy: 0.701693 \tValidation Accuracy: 0.652961\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 11\n",
      "\tTraining Loss: 21.622927 \tValidation Loss: 23.673101\n",
      "\tTraining Accuracy: 0.700282 \tValidation Accuracy: 0.662829\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 12\n",
      "\tTraining Loss: 20.654325 \tValidation Loss: 21.992065\n",
      "\tTraining Accuracy: 0.707334 \tValidation Accuracy: 0.682566\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 13\n",
      "\tTraining Loss: 19.184885 \tValidation Loss: 21.861976\n",
      "\tTraining Accuracy: 0.729196 \tValidation Accuracy: 0.710526\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 14\n",
      "\tTraining Loss: 19.698226 \tValidation Loss: 21.902659\n",
      "\tTraining Accuracy: 0.722849 \tValidation Accuracy: 0.695724\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 15\n",
      "\tTraining Loss: 19.490668 \tValidation Loss: 21.694826\n",
      "\tTraining Accuracy: 0.721439 \tValidation Accuracy: 0.689145\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 16\n",
      "\tTraining Loss: 18.160308 \tValidation Loss: 21.454748\n",
      "\tTraining Accuracy: 0.752468 \tValidation Accuracy: 0.713816\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 17\n",
      "\tTraining Loss: 17.085536 \tValidation Loss: 20.861137\n",
      "\tTraining Accuracy: 0.765162 \tValidation Accuracy: 0.718750\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 18\n",
      "\tTraining Loss: 17.437239 \tValidation Loss: 20.231948\n",
      "\tTraining Accuracy: 0.754584 \tValidation Accuracy: 0.717105\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 19\n",
      "\tTraining Loss: 16.367353 \tValidation Loss: 20.006670\n",
      "\tTraining Accuracy: 0.770099 \tValidation Accuracy: 0.731908\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 20\n",
      "\tTraining Loss: 16.720014 \tValidation Loss: 20.620687\n",
      "\tTraining Accuracy: 0.779972 \tValidation Accuracy: 0.722039\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 21\n",
      "\tTraining Loss: 15.668934 \tValidation Loss: 19.569775\n",
      "\tTraining Accuracy: 0.789140 \tValidation Accuracy: 0.726974\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 22\n",
      "\tTraining Loss: 14.582234 \tValidation Loss: 19.658714\n",
      "\tTraining Accuracy: 0.808886 \tValidation Accuracy: 0.726974\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 23\n",
      "\tTraining Loss: 14.789362 \tValidation Loss: 19.115213\n",
      "\tTraining Accuracy: 0.803244 \tValidation Accuracy: 0.745066\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 24\n",
      "\tTraining Loss: 13.619555 \tValidation Loss: 19.812444\n",
      "\tTraining Accuracy: 0.825811 \tValidation Accuracy: 0.725329\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 25\n",
      "\tTraining Loss: 14.007281 \tValidation Loss: 19.959100\n",
      "\tTraining Accuracy: 0.818759 \tValidation Accuracy: 0.707237\n",
      "trigger times: 2\n",
      "\n",
      "running epoch: 26\n",
      "\tTraining Loss: 13.300323 \tValidation Loss: 18.595576\n",
      "\tTraining Accuracy: 0.818759 \tValidation Accuracy: 0.758224\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 27\n",
      "\tTraining Loss: 12.945186 \tValidation Loss: 18.076874\n",
      "\tTraining Accuracy: 0.837800 \tValidation Accuracy: 0.758224\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 28\n",
      "\tTraining Loss: 12.641193 \tValidation Loss: 18.750839\n",
      "\tTraining Accuracy: 0.845557 \tValidation Accuracy: 0.731908\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 29\n",
      "\tTraining Loss: 12.110568 \tValidation Loss: 17.882071\n",
      "\tTraining Accuracy: 0.841326 \tValidation Accuracy: 0.766447\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 30\n",
      "\tTraining Loss: 11.077201 \tValidation Loss: 17.988983\n",
      "\tTraining Accuracy: 0.868124 \tValidation Accuracy: 0.764803\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 31\n",
      "\tTraining Loss: 10.777631 \tValidation Loss: 17.650896\n",
      "\tTraining Accuracy: 0.866714 \tValidation Accuracy: 0.781250\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 32\n",
      "\tTraining Loss: 11.095058 \tValidation Loss: 19.975581\n",
      "\tTraining Accuracy: 0.858251 \tValidation Accuracy: 0.748355\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 33\n",
      "\tTraining Loss: 10.479126 \tValidation Loss: 17.952484\n",
      "\tTraining Accuracy: 0.874471 \tValidation Accuracy: 0.754934\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 34\n",
      "\tTraining Loss: 10.405487 \tValidation Loss: 18.488128\n",
      "\tTraining Accuracy: 0.866714 \tValidation Accuracy: 0.740132\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 35\n",
      "\tTraining Loss: 9.853031 \tValidation Loss: 17.698539\n",
      "\tTraining Accuracy: 0.880818 \tValidation Accuracy: 0.781250\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 36\n",
      "\tTraining Loss: 9.402532 \tValidation Loss: 17.369610\n",
      "\tTraining Accuracy: 0.880818 \tValidation Accuracy: 0.754934\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 37\n",
      "\tTraining Loss: 9.151985 \tValidation Loss: 18.458569\n",
      "\tTraining Accuracy: 0.895628 \tValidation Accuracy: 0.769737\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 38\n",
      "\tTraining Loss: 8.608106 \tValidation Loss: 16.827022\n",
      "\tTraining Accuracy: 0.896333 \tValidation Accuracy: 0.784539\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 39\n",
      "\tTraining Loss: 7.804166 \tValidation Loss: 18.271426\n",
      "\tTraining Accuracy: 0.913258 \tValidation Accuracy: 0.776316\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 40\n",
      "\tTraining Loss: 7.541205 \tValidation Loss: 19.079669\n",
      "\tTraining Accuracy: 0.904090 \tValidation Accuracy: 0.746711\n",
      "trigger times: 2\n",
      "\n",
      "running epoch: 41\n",
      "\tTraining Loss: 7.851274 \tValidation Loss: 17.088249\n",
      "\tTraining Accuracy: 0.905501 \tValidation Accuracy: 0.776316\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 42\n",
      "\tTraining Loss: 8.280517 \tValidation Loss: 17.675494\n",
      "\tTraining Accuracy: 0.898449 \tValidation Accuracy: 0.777961\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 43\n",
      "\tTraining Loss: 7.846867 \tValidation Loss: 16.830851\n",
      "\tTraining Accuracy: 0.909027 \tValidation Accuracy: 0.792763\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 44\n",
      "\tTraining Loss: 7.192538 \tValidation Loss: 18.071371\n",
      "\tTraining Accuracy: 0.911142 \tValidation Accuracy: 0.756579\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 45\n",
      "\tTraining Loss: 6.524753 \tValidation Loss: 17.763726\n",
      "\tTraining Accuracy: 0.929478 \tValidation Accuracy: 0.764803\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 46\n",
      "\tTraining Loss: 6.140696 \tValidation Loss: 18.108900\n",
      "\tTraining Accuracy: 0.932299 \tValidation Accuracy: 0.782895\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 47\n",
      "\tTraining Loss: 6.935915 \tValidation Loss: 19.801233\n",
      "\tTraining Accuracy: 0.913258 \tValidation Accuracy: 0.771382\n",
      "trigger times: 2\n",
      "\n",
      "running epoch: 48\n",
      "\tTraining Loss: 6.095117 \tValidation Loss: 17.297525\n",
      "\tTraining Accuracy: 0.924542 \tValidation Accuracy: 0.771382\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 49\n",
      "\tTraining Loss: 5.605068 \tValidation Loss: 16.962908\n",
      "\tTraining Accuracy: 0.941467 \tValidation Accuracy: 0.784539\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 50\n",
      "\tTraining Loss: 5.674336 \tValidation Loss: 17.690498\n",
      "\tTraining Accuracy: 0.938646 \tValidation Accuracy: 0.789474\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 51\n",
      "\tTraining Loss: 5.209833 \tValidation Loss: 18.398630\n",
      "\tTraining Accuracy: 0.938646 \tValidation Accuracy: 0.769737\n",
      "trigger times: 2\n",
      "\n",
      "running epoch: 52\n",
      "\tTraining Loss: 5.426013 \tValidation Loss: 17.697443\n",
      "\tTraining Accuracy: 0.934415 \tValidation Accuracy: 0.791118\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 53\n",
      "\tTraining Loss: 5.269515 \tValidation Loss: 17.538231\n",
      "\tTraining Accuracy: 0.942172 \tValidation Accuracy: 0.812500\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 4.966736 \tValidation Loss: 18.388692\n",
      "\tTraining Accuracy: 0.937236 \tValidation Accuracy: 0.787829\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 55\n",
      "\tTraining Loss: 4.566080 \tValidation Loss: 21.372110\n",
      "\tTraining Accuracy: 0.953456 \tValidation Accuracy: 0.754934\n",
      "trigger times: 2\n",
      "\n",
      "running epoch: 56\n",
      "\tTraining Loss: 4.544109 \tValidation Loss: 17.704113\n",
      "\tTraining Accuracy: 0.943583 \tValidation Accuracy: 0.782895\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 57\n",
      "\tTraining Loss: 4.703217 \tValidation Loss: 17.063801\n",
      "\tTraining Accuracy: 0.941467 \tValidation Accuracy: 0.800987\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 58\n",
      "\tTraining Loss: 5.058656 \tValidation Loss: 16.608723\n",
      "\tTraining Accuracy: 0.936530 \tValidation Accuracy: 0.797697\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 59\n",
      "\tTraining Loss: 4.114873 \tValidation Loss: 17.173933\n",
      "\tTraining Accuracy: 0.953456 \tValidation Accuracy: 0.812500\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 60\n",
      "\tTraining Loss: 4.790912 \tValidation Loss: 15.360794\n",
      "\tTraining Accuracy: 0.943583 \tValidation Accuracy: 0.802632\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 61\n",
      "\tTraining Loss: 3.979668 \tValidation Loss: 16.549631\n",
      "\tTraining Accuracy: 0.952750 \tValidation Accuracy: 0.807566\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 62\n",
      "\tTraining Loss: 3.350155 \tValidation Loss: 20.391170\n",
      "\tTraining Accuracy: 0.968265 \tValidation Accuracy: 0.750000\n",
      "trigger times: 2\n",
      "\n",
      "running epoch: 63\n",
      "\tTraining Loss: 3.937014 \tValidation Loss: 17.934214\n",
      "\tTraining Accuracy: 0.954866 \tValidation Accuracy: 0.800987\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 64\n",
      "\tTraining Loss: 3.203895 \tValidation Loss: 18.116693\n",
      "\tTraining Accuracy: 0.968970 \tValidation Accuracy: 0.794408\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 65\n",
      "\tTraining Loss: 3.818904 \tValidation Loss: 17.613738\n",
      "\tTraining Accuracy: 0.960508 \tValidation Accuracy: 0.799342\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 66\n",
      "\tTraining Loss: 3.245251 \tValidation Loss: 19.378624\n",
      "\tTraining Accuracy: 0.965444 \tValidation Accuracy: 0.768092\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 67\n",
      "\tTraining Loss: 3.327096 \tValidation Loss: 16.451430\n",
      "\tTraining Accuracy: 0.959803 \tValidation Accuracy: 0.804276\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 68\n",
      "\tTraining Loss: 3.257684 \tValidation Loss: 19.801835\n",
      "\tTraining Accuracy: 0.971791 \tValidation Accuracy: 0.799342\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 69\n",
      "\tTraining Loss: 2.793983 \tValidation Loss: 17.874465\n",
      "\tTraining Accuracy: 0.973202 \tValidation Accuracy: 0.796053\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 70\n",
      "\tTraining Loss: 3.059051 \tValidation Loss: 17.328675\n",
      "\tTraining Accuracy: 0.966150 \tValidation Accuracy: 0.812500\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 71\n",
      "\tTraining Loss: 3.195884 \tValidation Loss: 19.552431\n",
      "\tTraining Accuracy: 0.966150 \tValidation Accuracy: 0.797697\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 72\n",
      "\tTraining Loss: 3.185832 \tValidation Loss: 17.672352\n",
      "\tTraining Accuracy: 0.958392 \tValidation Accuracy: 0.787829\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 73\n",
      "\tTraining Loss: 2.838441 \tValidation Loss: 17.686849\n",
      "\tTraining Accuracy: 0.968970 \tValidation Accuracy: 0.800987\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 74\n",
      "\tTraining Loss: 2.612175 \tValidation Loss: 17.453632\n",
      "\tTraining Accuracy: 0.975317 \tValidation Accuracy: 0.807566\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 75\n",
      "\tTraining Loss: 2.239561 \tValidation Loss: 18.896306\n",
      "\tTraining Accuracy: 0.972496 \tValidation Accuracy: 0.817434\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 76\n",
      "\tTraining Loss: 2.701221 \tValidation Loss: 18.600900\n",
      "\tTraining Accuracy: 0.963329 \tValidation Accuracy: 0.819079\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 77\n",
      "\tTraining Loss: 2.331521 \tValidation Loss: 19.879133\n",
      "\tTraining Accuracy: 0.978138 \tValidation Accuracy: 0.786184\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 78\n",
      "\tTraining Loss: 2.293795 \tValidation Loss: 18.658009\n",
      "\tTraining Accuracy: 0.977433 \tValidation Accuracy: 0.810855\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 79\n",
      "\tTraining Loss: 2.112801 \tValidation Loss: 18.379576\n",
      "\tTraining Accuracy: 0.976728 \tValidation Accuracy: 0.812500\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 80\n",
      "\tTraining Loss: 2.474703 \tValidation Loss: 19.035532\n",
      "\tTraining Accuracy: 0.976728 \tValidation Accuracy: 0.804276\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 81\n",
      "\tTraining Loss: 2.510743 \tValidation Loss: 18.829080\n",
      "\tTraining Accuracy: 0.976728 \tValidation Accuracy: 0.799342\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 82\n",
      "\tTraining Loss: 2.164756 \tValidation Loss: 18.457231\n",
      "\tTraining Accuracy: 0.976023 \tValidation Accuracy: 0.822368\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 83\n",
      "\tTraining Loss: 2.074899 \tValidation Loss: 19.814052\n",
      "\tTraining Accuracy: 0.977433 \tValidation Accuracy: 0.792763\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 84\n",
      "\tTraining Loss: 2.450681 \tValidation Loss: 19.914688\n",
      "\tTraining Accuracy: 0.977433 \tValidation Accuracy: 0.797697\n",
      "trigger times: 2\n",
      "\n",
      "running epoch: 85\n",
      "\tTraining Loss: 1.765953 \tValidation Loss: 20.054327\n",
      "\tTraining Accuracy: 0.987306 \tValidation Accuracy: 0.799342\n",
      "trigger times: 3\n",
      "\n",
      "running epoch: 86\n",
      "\tTraining Loss: 1.913453 \tValidation Loss: 19.004316\n",
      "\tTraining Accuracy: 0.982370 \tValidation Accuracy: 0.819079\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 87\n",
      "\tTraining Loss: 1.740241 \tValidation Loss: 17.926172\n",
      "\tTraining Accuracy: 0.980254 \tValidation Accuracy: 0.809211\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 88\n",
      "\tTraining Loss: 1.641378 \tValidation Loss: 18.787509\n",
      "\tTraining Accuracy: 0.985190 \tValidation Accuracy: 0.830592\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 89\n",
      "\tTraining Loss: 1.379539 \tValidation Loss: 17.831050\n",
      "\tTraining Accuracy: 0.988011 \tValidation Accuracy: 0.809211\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 90\n",
      "\tTraining Loss: 1.705157 \tValidation Loss: 17.649183\n",
      "\tTraining Accuracy: 0.986601 \tValidation Accuracy: 0.827303\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 91\n",
      "\tTraining Loss: 1.617390 \tValidation Loss: 20.678206\n",
      "\tTraining Accuracy: 0.980959 \tValidation Accuracy: 0.824013\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 92\n",
      "\tTraining Loss: 1.682457 \tValidation Loss: 19.626710\n",
      "\tTraining Accuracy: 0.989422 \tValidation Accuracy: 0.810855\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 93\n",
      "\tTraining Loss: 2.032893 \tValidation Loss: 19.606824\n",
      "\tTraining Accuracy: 0.976728 \tValidation Accuracy: 0.791118\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 94\n",
      "\tTraining Loss: 1.653695 \tValidation Loss: 19.545498\n",
      "\tTraining Accuracy: 0.983075 \tValidation Accuracy: 0.814145\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 95\n",
      "\tTraining Loss: 1.850697 \tValidation Loss: 22.011520\n",
      "\tTraining Accuracy: 0.978843 \tValidation Accuracy: 0.777961\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 96\n",
      "\tTraining Loss: 1.575448 \tValidation Loss: 19.131391\n",
      "\tTraining Accuracy: 0.985190 \tValidation Accuracy: 0.814145\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 97\n",
      "\tTraining Loss: 1.574603 \tValidation Loss: 21.196762\n",
      "\tTraining Accuracy: 0.983780 \tValidation Accuracy: 0.789474\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 98\n",
      "\tTraining Loss: 1.531949 \tValidation Loss: 18.563161\n",
      "\tTraining Accuracy: 0.985896 \tValidation Accuracy: 0.805921\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 99\n",
      "\tTraining Loss: 1.649909 \tValidation Loss: 22.739699\n",
      "\tTraining Accuracy: 0.979549 \tValidation Accuracy: 0.810855\n",
      "trigger times: 1\n",
      "\n",
      "running epoch: 100\n",
      "\tTraining Loss: 1.631217 \tValidation Loss: 19.956258\n",
      "\tTraining Accuracy: 0.980959 \tValidation Accuracy: 0.807566\n",
      "trigger times: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_ft_alexnet = models.alexnet(pretrained=True)\n",
    "num_ftrs = model_ft_alexnet.classifier[6].in_features\n",
    "model_ft_alexnet.classifier[6] = nn.Linear(num_ftrs,3)\n",
    "model_ft_alexnet=model_ft_alexnet.to(device)# 放入裝置\n",
    "print(model_ft_alexnet) # 列印新模型\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "train_dataloader, valid_dataloader = dataloader_prepare(folder='./ML_hw2/學生的training_data/',batch_size = batch_size)\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':model_ft_alexnet.parameters()}\n",
    "], lr=0.00001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "patience = 3\n",
    "\n",
    "model_ft_alexnet=train(model_ft_alexnet,n_epochs,train_dataloader,valid_dataloader,optimizer,criterion, batch_size , patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try wide_resnet50_2 finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8436d71cc7ef4cd7a054129b3391f810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "(1)：377.0 0.18608094768015795\n",
      "(2)：488.0 0.24086870681145114\n",
      "(3)：1161.0 0.573050345508391\n",
      "\n",
      "running epoch: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 10.76 GiB total capacity; 5.78 GiB already allocated; 75.25 MiB free; 5.93 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9f1ba057f678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel_ft_wide_resnet50_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft_wide_resnet50_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-9d3df8dd0a3b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, n_epochs, train_loader, valid_loader, optimizer, criterion, batch_size, patience)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;31m# calculate the batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1922\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m     )\n\u001b[1;32m   1925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 10.76 GiB total capacity; 5.78 GiB already allocated; 75.25 MiB free; 5.93 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model_ft_wide_resnet50_2 = models.wide_resnet50_2(pretrained=True)\n",
    "num_ftrs = model_ft_wide_resnet50_2.fc.in_features\n",
    "model_ft_wide_resnet50_2.fc = nn.Linear(num_ftrs,3)\n",
    "model_ft_wide_resnet50_2=model_ft_wide_resnet50_2.to(device)# 放入裝置\n",
    "print(model_ft_wide_resnet50_2) # 列印新模型\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "train_dataloader, valid_dataloader = dataloader_prepare(folder='./ML_hw2/學生的training_data/',batch_size = batch_size)\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':model_ft_wide_resnet50_2.parameters()}\n",
    "], lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "patience = 4\n",
    "\n",
    "model_ft_wide_resnet50_2=train(model_ft_wide_resnet50_2,n_epochs,train_dataloader,valid_dataloader,optimizer,criterion, batch_size , patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try vgg16 finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_2514_20200703_094952_9.9.jpg\n",
      "No file for path : ./ML_hw2/學生的training_data/B/17871372_B_7322_20200925_114302_8.8.jpg\n",
      "\n",
      "data size: \n",
      "2026 2026\n",
      "(1)：377.0 0.18608094768015795\n",
      "(2)：488.0 0.24086870681145114\n",
      "(3)：1161.0 0.573050345508391\n",
      "\n",
      "running epoch: 1\n",
      "\tTraining Loss: 31.734260 \tValidation Loss: 30.213683\n",
      "\tTraining Accuracy: 0.559238 \tValidation Accuracy: 0.575658\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 2\n",
      "\tTraining Loss: 30.152368 \tValidation Loss: 29.694429\n",
      "\tTraining Accuracy: 0.559944 \tValidation Accuracy: 0.582237\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 3\n",
      "\tTraining Loss: 29.212920 \tValidation Loss: 28.951340\n",
      "\tTraining Accuracy: 0.578984 \tValidation Accuracy: 0.578947\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 4\n",
      "\tTraining Loss: 28.468965 \tValidation Loss: 28.171375\n",
      "\tTraining Accuracy: 0.590973 \tValidation Accuracy: 0.587171\n",
      "trigger times: 0\n",
      "\n",
      "running epoch: 5\n"
     ]
    }
   ],
   "source": [
    "model_ft_vgg16 = models.vgg16(pretrained=True)\n",
    "num_ftrs = model_ft_vgg16.classifier[6].in_features\n",
    "model_ft_vgg16.classifier[6] = nn.Linear(num_ftrs,3)\n",
    "model_ft_vgg16=model_ft.to(device)# 放入裝置\n",
    "print(model_ft_vgg16) # 列印新模型\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "train_dataloader, valid_dataloader = dataloader_prepare(folder='./ML_hw2/學生的training_data/',batch_size = batch_size)\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':model_ft.parameters()}\n",
    "], lr=0.00001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "patience = 4\n",
    "\n",
    "model_ft_vgg16=train(model_ft_vgg16,n_epochs,train_dataloader,valid_dataloader,optimizer,criterion, batch_size , patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_ft, \"./hw2_classification\")\n",
    "# model_ft = torch.load('./hw2_classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print testing data result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1ae9ba8ece4e7cba6d0c84ac7cccab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data size: \n",
      "testing數量：110\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183b6badd49648beb0367aaaab97adbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=110.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [64, 3, 11, 11], but got 3-dimensional input of size [3, 224, 224] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a1b3b6797ecd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mpred_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/alexnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 346\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 3, 11, 11], but got 3-dimensional input of size [3, 224, 224] instead"
     ]
    }
   ],
   "source": [
    "testing_path = []\n",
    "folder = './ML_hw2/學生的testing_data/'\n",
    "slice_csv = 'testing_data'#提取testing_data\n",
    "csv_path = f'{folder}{slice_csv}.csv'\n",
    "resize_folder = f'{folder}resize/'\n",
    "if not os.path.isdir(resize_folder):\n",
    "    os.makedirs(resize_folder)\n",
    "with open(csv_path, 'r', encoding='utf8') as f:        \n",
    "    f.readline()\n",
    "    for line in tqdm(f):\n",
    "        clean_line = line.replace('\\n', '').replace('\\ufeff', '').split(',')\n",
    "        # [id, light, ground_truth, grade]\n",
    "        curr_img_path = f'{folder}{slice_csv}/{clean_line[0]}'\n",
    "        new_img_path = f'{resize_folder}{clean_line[0]}'\n",
    "        if not os.path.isfile(curr_img_path):\n",
    "            print(curr_img_path)\n",
    "            print('catch')\n",
    "            continue\n",
    "        if not os.path.isfile(new_img_path):\n",
    "            img = cv2.imread(curr_img_path, cv2.IMREAD_COLOR)\n",
    "            img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "            cv2.imwrite(new_img_path, img)\n",
    "        testing_path.append(new_img_path)\n",
    "print('data size: ')\n",
    "print(f'testing數量：{len(testing_path)}')\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "model_ft.eval()\n",
    "pred_label=[]\n",
    "for path in tqdm(testing_path):\n",
    "    img = Image.open(path)\n",
    "    img = transform(img)\n",
    "    with torch.no_grad(): \n",
    "        output=model_ft(img)\n",
    "    pred = output.data.max(dim = 1, keepdim = True)[1]\n",
    "    pred_label.append(int(pred))\n",
    "print(pred_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
